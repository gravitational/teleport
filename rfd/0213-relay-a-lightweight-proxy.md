---
authors: Edoardo Spadolini (edoardo.spadolini@goteleport.com)
state: draft
---

# RFD 213 - Relay, a lightweight tier 2 proxy

## What

A new cluster component, called Relay, that acts as a lightweight proxy, receiving reverse tunnel connections from agents and client connections, and routing connections from clients to resources without the need for the connection to go back and forth through the broader control plane.

## Why

Geographically distributed Teleport proxies are quite good at reducing latency when accessing resources in large, widespread clusters; however, for applications such as large data transfers in situations where the originator and the receiver of the connection are physically in the same datacenter, with a high performance network (that has no ingress egress costs) it can be very beneficial to arrange for connections to not go through the internet (or some other slow and/or expensive network link) to reach the Teleport control plane just to immediately get forwarded back through the same link.

On-prem setups can sometimes make use of trusted clusters in such scenarios - they're not supported at all in Teleport cloud - to allow connections to stay limited to a given datacenter, office, geographical region (from here on out, a "subenvironment"), but a trusted cluster setup requires a full, independent Teleport leaf cluster with its own set of permissions, with separate auditing and recordiqng, and using the leaf cluster directly with credentials from the parent cluster is only partially supported and getting credentials from the leaf generally requires duplicating users, SSO connectors or bots.

This RFD proposes an alternative that's entirely focused on connection topology, a peripheral agent service that serves as a lightweight, tier 2 proxy, called a Relay. A Teleport instance running the Relay service will itself connect to the control plane like any other agent, and it will receive and manage reverse tunnel connections from agents in the same subenvironment, as well as receive and (passively) route connections from clients to the agents.

## Details

### Relays, relay groups and the intended relay deployment

A Relay is a Teleport instance running the `relay_service`, with host credentials for the builtin role `Relay` (similarly to all other host credentials). The relay service is statically configured to self-report a relay group name (a short name, unique to the relays of that group that are assumed to all be in the same subenvironment), a relay group subenvironment-public address (a TCP load balancer or a DNS load balancer accessible and/or resolvable by agents and clients of the subenvironment, expected to point at all the relays of the same group), and an individual subenvironment-public address (an address - or a pair of IPv4/IPv6 addresses? - at which the Teleport instance is directly reachable by agents in the subenvironment and other relays in the same relay group). These bits of information are going to be part of the heartbeat resource of the relay itself for inspection and debugging, and the group name and individual address will be used by relays of the same group for mutual discovery and peering.

The configuration also includes an agent connection count number that works the same as the agent connection count for proxy peering (i.e., with an agent connection count of 2 it's expected that there will always be at least 2 relays in the group and agents will maintain reverse tunnels to 2 different relays in the group). The count will also be included in the resource heartbeat for debugging purposes.

Relays will route connections from clients to agents within the same subenvironment, as well as client connections from the broader Teleport cluster (from a proxy through the relay's own reverse tunnels). Client connections from the subenvironment to the relays are never going to be forwarded to the broader cluster if the agent that the client is trying to reach is not connected to the relay group. All Teleport API access (agent joining, heartbeats, audit logging and session recording upload, client login), as well as use of the Teleport web UI, will use the regular Teleport control plane directly, and will not be handled or forwarded by the relays - this is mostly to keep the required permissions for the `Relay` role as narrow as possible, but it will also simplify the implementation and it will avoid adding another potential point of failure for internal connectivity.

This RFD will detail support for (agentful) SSH and Kubernetes, other protocols are out of scope - what _can_ be supported in the future varies based on the protocol and the class of client used to access it: everything should be supportable through a local `tsh proxy` (and/or Teleport Connect and/or Teleport VNet), direct client connections for Database access might be possible, App access seems harder to support (as it's heavily tied to the web UI, in its direct client connection mode).

### Agent configuration and behavior

Agents will be configured with a `proxy_server` address pointed at the usual Proxy public address (which is still going to be used for the initial joining process and for all Teleport API access), but they can optionally also include a `relay_server` pointing at the relay group address. If such an address is specified, agents will not open connections for the regular reverse tunnel. At least in the initial implementation it will only be valid to specify `relay_server` with `proxy_server` (and thus with a "v3" configuration file).

Instead, agents configured to use a relay will connect to the relay group address to fetch the relay group discovery data (from any of the relays); this includes the relay group name, the agent connection count, and a list of the currently known relays for the group. They will then open reverse tunnel connections to different relays (up to the agent connection count) directly. This is different from the way that the regular reverse tunnel works, which requires agents to reach different proxies by opening new connections to the same public address to get randomly forwarded to different proxies behind a load balancer - connecting directly saves resources on the load balancer and the network, and it plays well with the intended usage scenario (agents and relays in the same subenvironment). Relays that are being gracefully terminated will report themselves as such to the connected agents (for both existing and new connections), and agents will not consider those relays for the purpose of filling the connection quota. Agents will include the relay group name and list of relay host IDs that they're connected to in their resources' heartbeats (the same way that proxy IDs are advertised in heartbeats today if proxy peering is enabled). Relays that are terminating will not be included in agents' heartbeats, reverse tunnel connections to terminating relays will be closed a few minutes after the last tunneled connection, and the relays themselves will wait a few minutes after the last tunneled connection before exiting. With appropriate grace times for disconnections as part of the graceful shutdown of the Teleport relay agents it should be straightforward to scale and upgrade a relay group without losing availability.

The discovery data will be occasionally re-fetched as a safeguard against broken relays resulting in isolation of an agent, but it's expected that any change to the available relays as a result of rollouts and scaling will be quickly propagated to all relays through the usual event fanout, and will then be broadcast to all the connected agents (similarly to how the regular reverse tunnel works). It is not practical to have all relay heartbeats replicated to individual agents through their own event watchers, since heartbeat-like resources cached by large volumes of agents are a performance problem for the event fanout as well as a noticeable load on the network. The periodic fetching of the discovery data is also how agents will come to learn about changes to the agent connection count, if any.

It should be possible for the relay reverse tunnels to support all services immediately, since no changes should be necessary for the connections tunneled from the Teleport control plane to the agents.

### Client behavior

The relay group address can be specified at `tsh login` time, and it can be explicitly specified and/or disabled as part of the invocation of `tsh ssh`, `tsh proxy ssh`, `tsh kube login` or `tsh proxy kube`, with either a command line option or an environment variable; as usual, the command line option overrides the envvar which overrides the configuration stored at login time. Access through machine ID will need explicit configuration as part of the "output" or "service" in use, and any need to rapidly switch between relay and non-relay routing can be served by creating two copies of a given output or service. At least in the first implementation of the client tooling side of this feature, there will be no support for automatic switching or fallback between relay and non-relay connection routing, as well as no safeguards against connecting through the broader control plane even if a relay could've been used.

Specifying a relay address at login time in Teleport Connect is left for future work, but a possible way to allow that could be an "Advanced..." button in the "Enter cluster address" modal, or some UI element to change settings for the active cluster after logging in. Editing the profile configuration file is also an option (albeit one that we don't necessarily want to encourage or support), as well as using `tsh ssh --relay <relayaddr> <user>@<host>` in a terminal tab instead of choosing a server to connect to in the list of resources.

All the relays of the group will serve client connections in the same way, so clients only ever need to interact directly with the single load balancer address - this is a requirement to support Kubernetes clients without a local proxy - and will do so depending on the required protocol.

For SSH, the existing proxy transport protocol will also be implemented by relays, which will run the same connection router that's used by the proxy to resolve a connection target into a host ID; however, unlike the actual proxies, relays will only ever route connections to resources that are available within the same relay group and will never open TCP connections to direct dial agents, and, to avoid giving relays any more privileges than they strictly require, agentless SSH servers and proxy recording mode are not going to be supported. For consistency with the router in the Teleport proxy, host ID resolution will happen against the full SSH inventory without filtering for nodes available in the subenvironment, and the usual rules will apply in case of an ambiguous match. The list of SSH servers shown with `tsh ls` will be updated to show `<-relay (<relayname>)` rather than `<-tunnel` for nodes connected to a relay, and similar UI changes will be applied to the web UI and Connect.

For Kubernetes access through a relay we will tweak the protocol to include the destination cluster name as part of the SNI of the connection, and we will tweak the host side credentials of Kubernetes service agents to also include a wildcard DNS SAN (with a suffix to be determined) to allow unmodified Kubernetes API clients to validate the server certificate of the Kubernetes service. Connections coming from the regular control plane of the cluster will remain unmodified. The relay will check the inventory of kubernetes clusters based on the name extracted from SNI, it will pick a host serving the requested cluster, and forward the TLS connection along without terminating it; similarly to SSH, the resolution will happen against the full inventory, and if the resolved host ID is not connected to the relay group then the connection will fail.

### Global connection routing and relay peering

Resource heartbeats from agents connected to relays will include a list of relay IDs; that list will be used by proxies to reference the relay heartbeats (which will include the proxy IDs that they're connected to); if a relay has an open tunnel to the proxy trying to open a connection, that relay will be used, otherwise the usual proxy peering rules apply when picking a peer proxy that has a tunnel to one of the relays. The proxy peering dial request will be extended to include an optional relay ID (valid when connecting to a host ID), and the peer proxy receiving the dial request will forward the connection to the specified relay through the reverse tunnel; the dial request in the reverse tunnel will include the actual target. The target must be available as a reverse tunnel to that relay directly. No further changes should be necessary to support connecting from a root cluster to a resource available through a relay in a leaf cluster, since the involvement of the trusted cluster connectivity stops at the cluster boundary already (without taking proxy peering into account, for example), and the proxy in the leaf cluster will establish a connection to the required node as it would for a connection originating in the same cluster.

Clients connecting to a relay group address will eventually result in a target host ID known to be available through some relay IDs; if the relay chosen by the loadbalancer is one of them (i.e. if the relay serving the connection request has a tunnel for that host ID) then the connection will just go through the reverse tunnel - otherwise, the connection will be forwarded to one of the listed relays, together with the original client source address, through a mechanism that's similar to the one of proxy peering - seeing as latency between relays is not a factor, however, it's likely better to open individual connections for each client connection rather than do the same multiplexing over a shared preexisting channel.

### Security considerations

Relays will be limited to passive connection routing, and will not ever be in possession of cleartext data or credentials. They will be allowed to fetch and watch the full resource inventory in the cluster to make routing decisions, but the routing decisions will only expose information that's equivalent (or similar enough) to the one exposed today as a result of routing for SSH (and Desktops, soon) in the proxy. The existence of a given kubernetes cluster by name is something that's currently not revealed to users that don't have access to the cluster, but it's the same information disclosure that we have accepted for SSH servers and desktops, and it allows us to not grant similar Kubernetes impersonation powers to the ones of the Proxy instances to every Relay.

The source IP information will be the one as seen by the relay (or rather the load balancer in front of the relay, forwarding the client IP info through a PROXY line as it's currently supported for most Teleport listeners), which means that the same IP address might be seen in multiple unrelated audit log events even if it's different sources in different subenvironments that just happened to use the same private IP address. This, however, is essentially just the dual problem to seeing the same public client IP from different machines behind NAT; and seeing as no local IP address will ever "leak" to the broader control plane, the use of relays will result in more granular and precise logging info.

Credentials with source IP pinning will not be usable through the relay, since the only source IP accessible at login time (let alone plausibly trustable by the control plane) is the public one as seen from the proxy, which is not accessible to the relay.
