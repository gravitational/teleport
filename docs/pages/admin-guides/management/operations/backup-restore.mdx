---
title: Backup and Restore
description: How to back up and restore your Teleport cluster state.
---

Operators must have a plan in place to back up self-hosted Teleport clusters.
While the Teleport Proxy Service and Teleport Agent services are stateless, you
should ensure that you can restore their configuration files. The Teleport Auth
Service manages state for the entire cluster, and it is critical that you can
back up its data. This guide explains the components of a Teleport Auth Service
deployment that must be backed up and lays out our recommended approach for
performing backups. 

(!docs/pages/includes/cloud/call-to-action.mdx!)

## Data to back up

You must back up the following Auth Service data. The cluster state backend is a
key-value store for dynamic resources, audit events, and other state data. The
session recording backend is an Amazon S3-compatible object store:

| What | Where |
| - | - |
| Local Users (not SSO) | Cluster state backend |
| Certificate Authorities | Cluster state backend |
| Dynamic resources ([more information below](#dynamic-resources)) | Cluster state backend |
| teleport.yaml | File system |
| teleport.service | File system |
| license.pem | File system |
| TLS key/certificate | File system or third-party service (e.g., AWS Certificate Manager)|
| Audit log | Cluster state backend |
| Session recordings | Session recording backend |

## Backing up Teleport backends

Your plan for backing up the Teleport cluster state and session recording
backends depends on the solution you use for each backend. The following table
includes instructions for backing up each backend solution. For backends not
listed here, consult the documentation for your backend:

| Backend | Recommended backup strategy |
| - | - |
| Local Filesystem | Back up the data directory (`/var/lib/teleport/` by default) |
| DynamoDB | [Follow AWS's guidelines for backup and restore](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html) |
| etcd | [Follow etcd's guidelines for disaster recovery](https://etcd.io/docs/v2/admin_guide) |
| Firestore | [Follow GCP's guidelines for automated backups](https://firebase.google.com/docs/database/backups) |
| Azure Database for PostgreSQL | [Follow Azure's guidelines for automated backups](https://learn.microsoft.com/en-us/azure/backup/backup-azure-database-postgresql) |

## Dynamic resources

Teleport uses [dynamic
resources](../../infrastructure-as-code/infrastructure-as-code.mdx) for roles,
local users, authentication connectors, and other configurations. We recommend
that you back up your configuration resources because they are essential to the
normal operation of your cluster.

### Using infrastructure as code (recommended)

We recommend managing dynamic resources as code using one of the following
tools, along with a version control system and continuous deployment pipeline
that applies your configurations automatically:

- [Teleport Terraform
  provider](../../infrastructure-as-code/terraform-provider/terraform-provider.mdx)
- [Teleport Kubernetes
  operator](../../infrastructure-as-code/teleport-operator/teleport-operator.mdx)

This way, you can ensure that all Teleport configurations in your cluster are
always up to date. Your source repository can act as a backup of your dynamic
configurations, and restoring is only a matter of applying these configurations.

### Using a state file

You can run a command to retrieve all resource manifests and print them to a
single file, the **state file**, then restart the Teleport Auth Service using
the state file to bootstrap the cluster.

#### Restarting the Auth Service with a state file

1. Log in to your Auth Service machine and run the following command:

   ```code
   $ tctl get all --with-secrets > state.yaml
   ```

1. Prepare a new uninitialized backend (make sure to port any non-default config
   values from the old config file):

   ```code
   $ mkdir fresh && cat > fresh.yaml << EOF
   teleport:
     data_dir: fresh
   EOF
   ```

1. Stop Teleport on each Auth Service instance.

1. Restart Teleport on each Auth Service instance with the fresh backend and the
   state file you created earlier:

   ```code
   $ sudo teleport start --config fresh.yaml --bootstrap state.yaml
   ```

1. From another terminal, verify that the state transferred correctly:

   ```code
   $ tctl --config fresh.yaml get all
   # <your state here>
   ```

#### Limitations

The `--bootstrap` flag has no effect, except when the Auth Service initializes
its backend on first startup, so it is safe for use in supervised/High
Availability contexts.

The `--bootstrap` flag doesn't re-trigger Trusted Cluster handshakes, so Trusted
Cluster resources need to be recreated manually.

All the same limitations around modifying the config file of an existing cluster
also apply to a new cluster being bootstrapped from the state of an old cluster:

  - Changing the cluster name will break your CAs. This will be caught and Teleport
    will refuse to start.
  - Some user authentication mechanisms (e.g. WebAuthn) require that the public
    endpoint of the Web UI remains the same. This cannot be caught by Teleport,
    so be careful!
  - Any Teleport Agent whose invite token is defined in the Auth Service's
    configuration file will be able to join automatically, but Agents that were
    added dynamically will need to be re-invited.

