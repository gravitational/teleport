---
title: Session Recording Summaries
description: Describes how to generate and view session recording summaries using a language model.
labels:
  - conceptual
  - identity-security
---

Teleport Identity Security allows you to generate and view session recording
summaries for shell and database sessions. This allows you to see at a glance
what your users do and estimate legitimacy of their actions before digging
deeper and reviewing the entire recording.

<Admonition type="warning">
This feature is powered by AI, and as such, its accuracy may vary.

Teleport doesn't currently support any spend controls; controlling the LLM
token budget is your responsibility.
</Admonition>

## Prerequisites

- A running Teleport Enterprise cluster v18.2.0 or later with Identity Security
  enabled.
- Access to OpenAI or OpenAI-compatible inference API, either public or
  self-hosted.

## How session recording summarization works

There are three fundamental concepts to that underpin session summarization:

- **Inference providers** do the heavy lifting of summarization. Currently,
  Teleport only supports OpenAI or compatible inference providers.
- **Inference models** indicate *how* sessions get summarized: they are
  Teleport configuration resources that provide model settings.
- **Inference policies** indicate *what* gets summarized and which inference
  model is used in given case.

After each session finishes, Teleport decides whether it should be summarized
or not by matching it against a set of inference policies. If a match is found,
Teleport uses an inference model configuration indicated by the policy to
generate a session recording summary using an inference provider. There is no
way to run this process on demand.

## Step 1: Access control

Every user who has access to given session recording also has access to its
summary through a `session` resource kind. The built-in `access` role allows
users to only view their sessions; in order to allow a user to view all
recordings and summaries, it's easiest to use the built-in `auditor` role.

In order to manage the inference configuration, you need a write access to
`inference_model`, `inference_secret`, and `inference_policy` resources, which
is granted by default through the built-in `editor` role.

You can either use the built-in roles or create a new one to make sure you have access to all necessary resources. To create a new role:

1. Create a `summarizer-admin.yaml` file with the following content:

   ```yaml
   kind: role
   metadata:
     name: summarizer-admin
   spec:
     allow:
       rules:
       - resources:
         - inference_model
         - inference_secret
         - inference_policy
         verbs:
         - read
         - list
         - create
         - update
         - delete
       - resources:
         - session
         verbs:
         - read
         - list
   version: v7
   ```

1. Apply your changes:

   ```code
   $ tctl create -f summarizer-admin.yaml
   ```

   (!docs/pages/includes/create-role-using-web.mdx!)

1. (!docs/pages/includes/add-role-to-user.mdx role="summarizer-admin"!)

## Step 2: Create the inference configuration resources

Let's start by defining an `inference_model` resource. It tells Teleport how to
summarize a session by pointing it to a language model API. In particular, it
contains an API-specific model name. Note that the *Teleport inference model
name* and *API-specific model name* are two different things. This allows, for
example, using the same OpenAI model with different temperature settings or
using different API gateways by creating multiple Teleport inference models
with the same OpenAI model name.

To communicate with OpenAI, an inference model needs an API key. It's stored
separately in an `inference_secret` resource and referenced by the
`inference_model`.

<Admonition type="warning">
To protect your API key, once written, an `inference_secret` value cannot be
retrieved through `tctl` or Teleport API.
</Admonition>

Finally, we add an `inference_policy` that routes sessions to inference models
using session kinds and optional custom filters. Here are some example
scenarios that can be expressed using inference policies:

- Only summarize sessions to your production resources using resource labels.
- Use different models for summarizing shell and database sessions.

To create the required configuration resources:

1. Create an `inference-config.yaml` file with the following content:

   ```yaml
   kind: inference_model
   version: v1
   metadata:
     name: ssh-summary-model
   spec:
     # "openai" is currently the only supported inference provider.
     openai:
       # In this scenario, where we connect to OpenAI directly, openai_model_id
       # needs to be a valid OpenAI model name.
       openai_model_id: gpt-4o
       # This value refers to the `inference_secret` resource name.
       api_key_secret_ref: openai-key
     # It's a good idea to limit the session length to prevent incurring cost for
     # sessions that were too long anyway to fit in the model's context window.
     # If unset, defaults to 200kB. See the Limitations section for more details.
     max_session_length_bytes: 190000

   ---

   kind: inference_secret
   version: v1
   metadata:
     name: openai-key
   spec:
     value: '<paste-your-openai-api-key-here>'

   ---

   kind: inference_policy
   version: v1
   metadata:
     name: shell-sessions
     description: Summarize all sessions using OpenAI
   spec:
     # Session kinds affected by this policy. Allowed values: "ssh", "k8s", "db".
     kinds:
     - ssh
     - k8s
     - db
     # Name of the `inference_model` resource that will be used for summarizing
     # these sessions.
     model: ssh-summary-model
   ```

1. Create the configuration resources in the backend:

   ```code
   $ tctl create -f inference-config.yaml
   ```

For full description of the inference configuration resources, see appropriate
sections of [Teleport Resources Reference](../reference/resources.mdx):

- [`inference_model`](../reference/resources.mdx#inference-model)
- [`inference_secret`](../reference/resources.mdx#inference-secret)
- [`inference_policy`](../reference/resources.mdx#inference-policy)

## Step 3: Conduct a session and view its summary

That's it! We are ready to conduct a test session. Connect to any of your
cluster resources using `tsh ssh`, `kubectl`, `tsh db connect`, or a web
terminal. Execute any command, quit the session, and go to **Audit -> Session
Recordings**. You should see the corresponding recording tile with a summary
button ![summary button](../../img/identity-security/summary-button.png).
Clicking the summary button will show the summary.

![Session recording list screen with a summary visible](../../img/identity-security/rec-list-with-summary.png)

<Admonition type="info">
It can take a while to generate a summary, and the time depends on multiple
factors, including session size, model, and its temperature. Usually it should
appear up to a minute after the session finishes.
</Admonition>

## Limitations

Currently, Teleport doesn't attempt to summarize sessions that are larger than
the model's context window. If after tokenization, the session turns out to be
larger than that, OpenAI will simply reject the request; the input tokens used
will be wasted. To prevent this, Teleport allows you to customize the maximum
size of session that may be sent to the inference provider by using the
`spec.max_session_length_bytes` field of the `inference_model` resource. Note
that there's no way to provide an exact number of bytes for a given model here;
in our experiments, we observed between 2 and 4 bytes per token, depending on
the model and input.

## Selecting sessions to summarize

Indiscriminately summarizing all sessions in your cluster can quickly drain
your wallet, so it's possible to be specific about which sessions will be
summarized and which model will be used for inference. One of these mechanisms
— selecting sessions by kind — was mentioned already in the above example. In
addition to matching by kind, it's also possible to add a custom filter to each
inference policy.

Filters are expressed using the
[Teleport predicate language](../reference/predicate-language.mdx#matching-session-summary-inference-policies).
If an inference policy contains a filter, it will match only sessions that
match one of the indicated kinds *and* the specified filter. A typical case
would be to pick only production resources for summarizing. The following
example policy only matches Kubernetes sessions in any of the production
clusters, picked by matching cluster labels:

```yaml
kind: inference_policy
version: v1
metadata:
  name: prod-k8s-sessions
  description: Summarize production Kubernetes sessions
spec:
  kinds:
  - k8s
  filter: 'equals(resource.metadata.labels["env"], "prod")'
  model: ssh-summary-model
```

It's possible to build predicates using information about target resource,
session, and user. The full list of supported properties can be found in the
[Predicate Language Reference](../reference/predicate-language.mdx#matching-session-summary-inference-policies).

## Using LiteLLM and other OpenAI-compatible proxies

Although the only officially supported inference provider is currently OpenAI,
it's also possible to use any OpenAI-compatible API for this purpose. For
example, you may want to route some or all sessions to another provider or even
host your language models on premises. One way to do it is using LiteLLM.
Here's what you need to do:

1. Set up [LiteLLM  Proxy Server (LLM
   Gateway)](https://docs.litellm.ai/docs/#litellm-proxy-server-llm-gateway) on a
   host accessible from the Teleport Auth server. 
2. Add a model of your choice to the LiteLLM configuration. In this example,
   let's call it `my-custom-model`.
3. Configure a Teleport inference model. Use `spec.openai.base_url` property to
   point Teleport to your LiteLLM instance and put `my-custom-model` in
   `spec.openai.openai_model_id`.
4. Add an `inference_secret` containing your LiteLLM master key.

Here is an example configuration:


```yaml
kind: inference_model
version: v1
metadata:
  name: lite-llm-summary-model
spec:
  openai:
    # In case of LiteLLM Proxy, the openai_model_id is the public model name as
    # configured in LiteLLM.
    openai_model_id: my-custom-model
    api_key_secret_ref: lite-llm-master-key
    base_url: http://llm-host:4000/

---

kind: inference_secret
version: v1
metadata:
  name: lite-llm-master-key
spec:
  # In case of LiteLLM, the API key is *not* your OpenAI key; instead, use your
  # LiteLLM master key.
  value: '<paste-your-master-key-here>'
```

## Monitoring

Teleport exposes a couple of useful Prometheus metrics that can be used to
monitor the session summarizer. Metrics are labeled by inference model name,
and in case of OpenAI errors — additionally with the OpenAI API error code. See
the [Teleport Metrics
reference](../reference/monitoring/metrics.mdx#session-recording-summarizer)
for details.