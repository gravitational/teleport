---
title: Set up Automatic Agent Upgrades
description: Describes how to set up automatic agent upgrades for self-hosted Teleport deployments.
---

Teleport supports automatic agent upgrades for systemd-based Linux distributions
using `apt`, `yum`, and `zypper` package managers, as well as Kubernetes
clusters. 

Teleport agents run an **upgrader** that queries a **version server** to
determine whether they are out of date. This guide describes how to set up your
infrastructure to support automatic upgrades. If you are a Teleport Cloud user
or run a version server already, return to the [Upgrading](../upgrading.mdx)
menu for the appropriate next steps to upgrade Teleport.

The [Automatic Update Architecture](../architecture/agent-update-management.mdx)
guide explains how automatic agent upgrades work in more detail.

<Admonition type="warning">
Systemd agents enrolled into automatic upgrades can only install versions
present in their package repositories. As Teleport 14 won't be published to
`stable/v13`, those agents will require manual intervention to be upgraded to
the next major version (adding a new APT/YUM/zypper repo for `stable/v14`).
Alternatively, you can use the `stable/rolling` channel, which contains
Teleport v13.3.2 forward, including future major releases.

</Admonition>

## Prerequisites

- Familiarity with the [Upgrading Compatibility Overview](./overview.mdx) guide,
  which describes the sequence in which to upgrade components of your cluster.
- Self-hosted Teleport cluster v13.0 or higher.
- (!docs/pages/includes/tctl-tsh-prerequisite.mdx!)
- (!docs/pages/includes/tctl.mdx!)

The version server must be hosted on a webserver with trusted TLS certificates
and reachable by all agents. You must have either:
  - A public Amazon S3 or Google Cloud Storage bucket.
  - A web server accessible from all agents with valid TLS certificates.

If you do not have an S3 or GCS bucket for your version server, this guide
provides an example of how to create one using Terraform. You can [install
Terraform](https://developer.hashicorp.com/terraform/downloads) to spin up a
demo version server along with this guide so you can get started with automatic
agent upgrades. Terraform is not required to set up a version server. 

- (!docs/pages/includes/tctl.mdx!)

## Step 1/5. Create release channel files

A **release channel** contains two pieces of information: the targeted version
and if the upgrade is critical. Updaters subscribe to a release channel and will
upgrade to the provided version during a **maintenance window** (which we will
configure later in this guide). If the upgrade is critical, updaters will ignore
the maintenance schedule and upgrade as soon as possible.

The version server is a static file server that responds to the following queries:

```code
$ curl https://<hosting-domain-and-path>/current/version
(=teleport.version=)

$ curl https://<hosting-domain-and-path>/current/critical
no
```

1. Create a project directory for the files we create in this guide:

   ```code
   $ mkdir version-server
   $ cd version-server
   ```

1. Create a directory for the new release channel `current`.

   ```code
   $ mkdir current/
   ```

1. Make the `current` release channel target the version (=teleport.version=):

   ```code
   $ echo -n "(=teleport.version=)" > current/version
   ```

1. And mark the upgrade as not critical:

   ```code
   $ echo -n "no" > current/critical
   ```

## Step 2/5. Create a Terraform configuration

1. In the project directory you created in the previous section, create a file
   called `main.tf` and populate it with the following content, depending on
   whether you will use Amazon S3 or Google Cloud Storage:

   <Tabs>
   <TabItem label="Amazon S3">

   In the configuration below, replace `REGION` with the name of the AWS region
   where you will deploy the version server:
   
   ```hcl
   terraform {
     required_providers {
       aws = {
         source  = "hashicorp/aws"
         version = "~> 5.0"
       }
     }
   }

   provider "aws" {
     region = "REGION"
   }

   resource "aws_s3_bucket" "version_server" {
     // Replace the bucket name to ensure it is unique
     bucket = "version-server"
   }
   
   resource "aws_s3_object" "version" {
     bucket = aws_s3_bucket.version_server.id
     key = "current/version"
     source = "${path.root}/current/version"
   }
   
   resource "aws_s3_object" "critical" {
     bucket = aws_s3_bucket.version_server.id
     key = "current/critical"
     source = "${path.root}/current/critical"
   }

   resource "aws_s3_bucket_policy" "version_server" {
     depends_on = [aws_s3_bucket_public_access_block.version_server]
     bucket = aws_s3_bucket.version_server.id
     policy = jsonencode({
       Version = "2012-10-17"
       Statement = [
         {
           Sid       = "GrantAnonymousReadPermissions"
           Principal = "*"
           Action = [
             "s3:GetObject",
           ]
           Effect   = "Allow"
           Resource = ["${aws_s3_bucket.version_server.arn}/*"]
         },
       ]
     })
   }
   
   resource "aws_s3_bucket_public_access_block" "version_server" {
     bucket = aws_s3_bucket.version_server.id
   
     block_public_acls       = false
     block_public_policy     = false
     ignore_public_acls      = false
     restrict_public_buckets = false
   }
   ```
   </TabItem>
   <TabItem label="Google Cloud Storage">

   In the configuration below, replace `PROJECT` and `REGION` with the Google
   Cloud project and region where you will deploy the version server:
   
   ```hcl
   terraform {
     required_providers {
       google = {
         source  = "hashicorp/google"
         version = "~> 5.0"
       }
     }
   }

   provider "google" {
     project     = "PROJECT"
     region      = "REGION"
   }

   resource "google_storage_bucket" "version_server" {
     // Change the location as needed. See:
     // https://cloud.google.com/storage/docs/locations
     location = "US-EAST1"
     // Replace this to ensure it is unique
     name = "version-server"
   }
   
   resource "google_storage_bucket_object" "version" {
     name   = "current/version"
     source = "${path.root}/current/version"
     bucket = google_storage_bucket.version_server.name
   }
   
   resource "google_storage_bucket_object" "critical" {
     name   = "current/critical"
     source = "${path.root}/current/critical"
     bucket = google_storage_bucket.version_server.name
   }

   data "google_iam_policy" "viewer" {
     binding {
       role = "roles/storage.objectViewer"
       members = [
           "allUsers",
       ] 
     }
   }
   
   resource "google_storage_bucket_iam_policy" "version_server" {
     bucket = google_storage_bucket.version_server.name
     policy_data = data.google_iam_policy.viewer.policy_data
   }
   ```
   </TabItem>
   </Tabs>

1. Make your cloud provider credentials available to Terraform. The method to
   use depends on your organization, e.g., pasting environment variables into
   your terminal.

1. Apply the configuration:

  ```code
  $ terraform init
  $ terraform apply
  ```

## Step 3/5. Configure the maintenance schedule

At this point the updaters can be configured to pull the version from the
release channel and upgrade the agents. In this step you'll configure a
maintenance schedule for the Teleport cluster that agents will use to determine
when to check for upgrades.

### Create a Teleport role

Create a Teleport role that can manage cluster maintenance configurations
through the `cluster_maintenance_config` dynamic resource. No preset Teleport
roles provide this ability, so you will need to create one.

1. Create a file called `cmc-editor.yaml` with the following content:

   ```yaml
   kind: role
   version: v7
   metadata:
     name: cmc-editor
   spec:
     allow:
       rules:
       - resources: ['cluster_maintenance_config']
         verbs: ['create', 'read', 'update', 'delete']
   ```

1. Create the role resource:

   ```code
   $ tctl create cmd-editor.yaml
   ```

1. Add the role to your Teleport user:

  (!docs/pages/includes/add-role-to-user.mdx role="cmc-editor"!)

### Create a cluster maintenance configuration

Create the following `cmc.yaml` manifest allowing maintenance on Monday,
Wednesday and Friday between 02:00 and 03:00 UTC.

(!docs/pages/includes/cluster-maintenance-config-spec.mdx!)

Finally, apply the manifest using `tctl`:

```code
$ tctl create cmc.yaml
maintenance window has been updated
```

## Step 4/5. Enroll Kubernetes agents in automatic upgrades

Now that you have deployed a version server, you can enroll agents in automatic
upgrades. This guide begins with agents deployed on Kubernetes. If all of your
agents run on Linux servers, you can skip to [Step
5](#step-55-enroll-linux-agents-in-automatic-upgrades).

### Install the agent upgrader Helm chart

This section assumes that the name of your `teleport-kube-agent` release is
`teleport-agent`, and that you have installed it in the `teleport` namespace.

1. Confirm you are using the Teleport Enterprise edition of the
   `teleport-kube-agent` chart. You should see the following when you query your
   `teleport-kube-agent` release:

   ```code
   $ helm -n <Var name="teleport" /> get values -n <Var name="teleport-agent" /> -o json | jq '.enterprise'
   true   
   ```

1. Add the following chart values to the values file for the
   `teleport-kube-agent` chart:

   ```yaml
   updater:
     enabled: true
     versionServer: <Var name="https://version.example.com" />
     releaseChannel: <Var name="release-channel" />
   ```

   Change <Var name="release-channel" /> to the name of the release channel you
   would like the agent to query for upgrades. For the current version, choose
   `current`.

1. Update the Helm chart release with the new values:

   ```code
   $ helm -n <Var name="teleport" />  upgrade <Var name="teleport-agent" /> teleport/teleport-kube-agent \
   --values=values.yaml \
   --version=<Var name="(=teleport.version=)" />
   ```

### Verify that the upgrader is working properly

1. You can validate the updater is running properly by checking if its pod is
   ready:

   ```code
   $ kubectl -n teleport-agent get pods
   NAME                               READY   STATUS    RESTARTS   AGE
   <your-agent-release>-0                         1/1     Running   0          14m
   <your-agent-release>-1                         1/1     Running   0          14m
   <your-agent-release>-2                         1/1     Running   0          14m
   <your-agent-release>-updater-d9f97f5dd-v57g9   1/1     Running   0          16m
   ```
   
1. Check for any deployment issues by checking the updater logs:
   
    ```code
    $ kubectl -n <Var name="teleport" /> logs deployment/<Var name="teleport-agent" />-updater
    2023-04-28T13:13:30Z	INFO	StatefulSet is already up-to-date, not updating.	{"controller": "statefulset", "controllerGroup": "apps", "controllerKind": "StatefulSet", "StatefulSet": {"name":"my-agent","namespace":"agent"}, "namespace": "agent", "name": "my-agent", "reconcileID": "10419f20-a4c9-45d4-a16f-406866b7fc05", "namespacedname": "agent/my-agent", "kind": "StatefulSet", "err": "no new version (current: \"v12.2.3\", next: \"v12.2.3\")"}
    ```

### Troubleshooting automatic agent upgrades on Kubernetes

The updater is a controller that periodically reconciles expected Kubernetes
resources with those in the cluster. The updater executes a reconciliation loop
every 30 minutes or in response to a Kubernetes event. If you don't want to wait
until the next reconciliation, you can trigger an event. 

1. Any deployment update will send an event, so you can trigger the upgrader by
   annotating the resource:

   ```code
   $ kubectl -n <Var name="teleport" /> annotate statefulset/<Var name="teleport-agent" /> 'debug.teleport.dev/trigger-event=1'
   ```

1. To suspend automatic upgrades for an agent, annotate the agent deployment
   with `teleport.dev/skipreconcile: "true"`, either by setting the
   `annotations.deployment` value in Helm, or by patching the deployment
   directly with `kubectl`.

## Step 5/5. Enroll Linux agents in automatic upgrades

This section shows you how to enroll Teleport agents running on Linux virtual or
bare-metal machines into automatic upgrades.

Follow these instructions on each of your Teleport agents.

### Install the agent upgrader

1. Ensure the Teleport repository is added and Teleport Enterprise is installed.

   To verify if the Teleport repository was added to the system, check if either of the follow files exist:

   ```code
   $ ls /etc/apt/sources.list.d/teleport.list
   # or
   $ ls /etc/yum.repos.d/teleport.repo
   ```

   The upgrader checks the repository for available releases, so make sure that
   it is up to date.
   
1. If the repository was added, make sure the Teleport binary installed on the
   agent can run the automatic upgrader:

   ```code
   $ which teleport-upgrade || echo "Install the upgrader"
   Install the upgrader
   ```

1. If the Teleport repository is not found, or the Teleport binary you installed
   does not include the upgrader, add the appropriate repository and reinstall
   Teleport.
   
   (!docs/pages/includes/install-linux-ent-self-hosted.mdx to-install="teleport-ent-updater" !)

### Configure the upgrader

1. Create the upgrade configuration directory:
   
   ```code
   $ sudo mkdir -p /etc/teleport-upgrade.d/
   ```
   
1. If you changed the agent user to run as non-root, create
   `/etc/teleport-upgrade.d/schedule` and grant ownership to your Teleport user.
   Otherwise, you can skip this step:
   
   ```code
   $ sudo touch /etc/teleport-upgrade.d/schedule
   $ sudo chown <Var name="your-teleport-user" /> /etc/teleport-upgrade.d/schedule
   ```
   
1. Configure the upgrader to connect to your custom version server and subscribe
   to the right release channel:
   
   ```code
   $ echo <Var name="version-server-url/path" />/<Var name="release-channel" /> | sudo tee /etc/teleport-upgrade.d/endpoint
   ```
   
   Make sure not to include `https://` as a prefix to the server address.

### Verify that the upgrader is working properly

1. Verify that the upgrader can see your version endpoint by checking for
   upgrades:

   ```code
   $ sudo teleport-upgrade dry-run
   ```
   
1. You should see one of the following messages, depending on the target version
   you are currently serving:
    
   ```text
   no upgrades available (1.2.3 == 1.2.3)
   an upgrade is available (1.2.3 -> 2.3.4)
   ```
    
   `teleport-upgrade` may display warnings about not having a valid upgrade
   schedule. This is expected immediately after install as the maintenance
   schedule might not be exported yet.

### Troubleshooting automatic agent upgrades on Linux

1. If an agent is not automatically upgraded, you can invoke the upgrader
   manually and look at its logs:

   ```code
   $ sudo teleport-upgrade run
   ```

1. To suspend automatic upgrades, disable the systemd timer:

   ```code
   $ sudo systemctl disable --now teleport-upgrade.timer
   ```

1. To enable and start the systemd timer after suspending:

   ```code
   $ sudo systemctl enable --now teleport-upgrade.timer
   ```

## Next steps

While this guide showed you how to apply a `cluster_maintenance_config` resource
using `tctl`, we recommend using infrastructure as code to maintain your
Teleport resources. See the
[`teleport_cluster_maintenance_config`](../reference/terraform-provider.mdx#teleport_cluster_maintenance_config)
for how to declare a cluster maintenance configuration with Terraform.
